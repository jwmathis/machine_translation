{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mxT0aooN7Hy"
      },
      "source": [
        "# Transfomer Model: Arabic to English Translation\n",
        "* TYPE OF MODEL: Using a transformer model. Is significant advancement in sequence-to-sequence tasks especially for natural language processing. Uses a self attention mechanism that allows the model to focus on different parts of the input sequence, capturing long-range dependencies more effectively than traditionall RNNs and LSTMs. Additionally it supports parallel processing, whihc enhances training speed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUG98b5VjHld"
      },
      "source": [
        "### 1. Importing Dependencies/Installing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.2.0\n",
            "  Using cached torch-2.2.0-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
            "Collecting torchtext==0.16.2\n",
            "  Using cached torchtext-0.16.2-cp312-cp312-win_amd64.whl.metadata (7.5 kB)\n",
            "Collecting filelock (from torch==2.2.0)\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.0)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.2.0)\n",
            "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.2.0)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.2.0)\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch==2.2.0)\n",
            "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting tqdm (from torchtext==0.16.2)\n",
            "  Using cached tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting requests (from torchtext==0.16.2)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting numpy (from torchtext==0.16.2)\n",
            "  Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.16.2)\n",
            "  Using cached torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting urllib3>=1.25 (from torchdata==0.7.1->torchtext==0.16.2)\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0)\n",
            "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.16.2)\n",
            "  Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchtext==0.16.2)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchtext==0.16.2)\n",
            "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\john.mathis.sern\\desktop\\sse672\\machine_translation\\.venv\\lib\\site-packages (from tqdm->torchtext==0.16.2) (0.4.6)\n",
            "Using cached torch-2.2.0-cp312-cp312-win_amd64.whl (198.5 MB)\n",
            "Using cached torchtext-0.16.2-cp312-cp312-win_amd64.whl (1.9 MB)\n",
            "Using cached torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
            "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 1.3/12.6 MB 7.4 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.6/12.6 MB 18.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.5/12.6 MB 21.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.6/12.6 MB 20.2 MB/s eta 0:00:00\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "Using cached tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, torchdata, torchtext\n",
            "Successfully installed MarkupSafe-3.0.2 certifi-2024.8.30 charset-normalizer-3.4.0 filelock-3.16.1 fsspec-2024.10.0 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.2 requests-2.32.3 sympy-1.13.3 torch-2.2.0 torchdata-0.7.1 torchtext-0.16.2 tqdm-4.66.6 typing-extensions-4.12.2 urllib3-2.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2.0 torchtext==0.16.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\john.mathis.sern\\desktop\\sse672\\machine_translation\\.venv\\lib\\site-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\john.mathis.sern\\desktop\\sse672\\machine_translation\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
            "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Installing collected packages: regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "installed torch==2.0.1 and torchtext==0.15.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\john.mathis.sern\\desktop\\sse672\\machine_translation\\.venv\\lib\\site-packages (2.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement 1.26.0 (from versions: none)\n",
            "ERROR: No matching distribution found for 1.26.0\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy 1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d4_XbNx4h3pu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"C:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
            "    self._run_once()\n",
            "  File \"C:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
            "    handle._run()\n",
            "  File \"C:\\Program Files\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\John.Mathis.SERN\\AppData\\Local\\Temp\\ipykernel_14200\\1163862124.py\", line 1, in <module>\n",
            "    import torch # for building neural network and other functions\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "c:\\Users\\John.Mathis.SERN\\Desktop\\SSE672\\machine_translation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;66;03m# for tokenization\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset \u001b[38;5;66;03m# Hugging Face Library to import datasets easily\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# import ipdb # for debugging\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
          ]
        }
      ],
      "source": [
        "import torch # for building neural network and other functions\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader # used inplace of BucketIterator\n",
        "import torch.optim as optim # optimizer\n",
        "import nltk # for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset # Hugging Face Library to import datasets easily\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# import ipdb # for debugging\n",
        "from torchtext.data.metrics import bleu_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEizTgqujO0J"
      },
      "source": [
        "### Step 1. Data Preprocessing\n",
        "* DATA PREPROCESSING: sourced out a dataset from Hugging Face and splitit into training and tests setsw. The toeknization process employs NLTK to break down Arabic and English text into manageable tokens. Once tokenized, the text is converted into numerical representations using custom built vocabularies, that facilitate the training of the transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOQI8-Vcq2On",
        "outputId": "8059256b-b90f-4e98-e51c-745062bdf4cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['arabic', 'english'],\n",
            "        num_rows: 65043\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['arabic', 'english'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Import the dataset from Hugging Face\n",
        "ds = load_dataset(\"mohamed-khalil/ATHAR\")\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 65043/65043 [00:01<00:00, 51331.42 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 35094.67 examples/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming the dataset has 'train' and 'test' splits and you're interested in lowercasing the 'src' and 'trg' fields\n",
        "def lowercase_text(example):\n",
        "    example['arabic'] = example['arabic'].lower()  # Lowercase the source text\n",
        "    example['english'] = example['english'].lower()  # Lowercase the target text\n",
        "    return example\n",
        "\n",
        "# Apply the lowercasing to the train and test sets\n",
        "ds['train'] = ds['train'].map(lowercase_text)\n",
        "ds['test'] = ds['test'].map(lowercase_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNw6iG7u-Ykw",
        "outputId": "41b96d75-5e3d-4764-bcd4-3c252da20590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size3: 52034\n",
            "Validation set size3: 13009\n",
            "Test set size: 1000\n"
          ]
        }
      ],
      "source": [
        "# Split the data\n",
        "# HuggingFace already has this split into a 'train' set and a 'test' set\n",
        "full_train_dataset = ds['train']\n",
        "\n",
        "# Split the train_dataset into training and validation sets (80% train, 20% validation)\n",
        "train_split = full_train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "# Access the new sets\n",
        "train_data = train_split['train']\n",
        "valid_data = train_split['test']\n",
        "#test_data = ds['test']\n",
        "test_data = ds['test']\n",
        "\n",
        "print(f\"Training set size3: {len(train_data)}\")\n",
        "print(f\"Validation set size3: {len(valid_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omrHwg0AjOKO",
        "outputId": "4a6f3b2a-6eb9-4f67-d2d1-e66e74d5d5b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to C:\\Users\\John\n",
            "[nltk_data]     Wesley\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Defining Tokenization functions\"\"\"\n",
        "nltk.download('punkt_tab') # downloads the language models for tokenizing\n",
        "\n",
        "# functions for tokenizing using nltk library; not necessary since word_tokenize() is a nice compact function, but lets you at least renname it\n",
        "def tokenize_arabic(text):\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode('utf-8')  # Adjust encoding as needed\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode('utf-8')  # Adjust encoding as needed\n",
        "    return word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Cmjc2uiNXE",
        "outputId": "2fd8cc70-2218-4d10-a993-c17f09dcf77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arabic tokens:  ['مرحبا', 'بالعالم', '!', 'هذا', 'هو', 'اختباري', 'الأول', '.']\n",
            "English tokens:  ['Hello', 'world', '!', 'This', 'is', 'my', 'first', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "# Block for Testing Tokenization\n",
        "arabic_text = \"مرحبا بالعالم! هذا هو اختباري الأول.\"\n",
        "arabic_tokens = tokenize_arabic(arabic_text)\n",
        "print(\"Arabic tokens: \", arabic_tokens)\n",
        "\n",
        "english_text = \"Hello world! This is my first test.\"\n",
        "english_tokens = tokenize_eng(english_text)\n",
        "print(\"English tokens: \", english_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "PfP1FiQWlGUN"
      },
      "outputs": [],
      "source": [
        "\"\"\"Preprocessing Functions\"\"\"\n",
        "# Convert text to lowercase, tokenize it and add <sos> and <eos> to tokens\n",
        "def process_text(text, tokenizer):\n",
        "  if isinstance(text, bytes):  # Check if text is in bytes\n",
        "      text = text.decode('utf-8')  # Decode bytes to string\n",
        "  tokens = tokenizer(text)\n",
        "  tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
        "  return tokens\n",
        "\n",
        "# yields processed tokens for each text in the dataset\n",
        "# is a generator that will pull one sentence at a time from dataset and tokenize the sentence\n",
        "def yield_tokens(data_iter, tokenizer):\n",
        "  for text in data_iter:\n",
        "    yield process_text(text, tokenizer)\n",
        "\n",
        "# Converts tokens into numerical representations using a vocab\n",
        "def numericalize(text, tokenizer, vocab):\n",
        "  if isinstance(text, bytes):\n",
        "    text = text.decode('utf-8')\n",
        "  tokens = process_text(text, tokenizer)\n",
        "  # return [vocab[token] for token in tokens]\n",
        "  return [vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-lBYVh1l6Y7",
        "outputId": "26cd454d-c19c-4e50-eaa1-faa747defae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4, 5, 6, 2]\n"
          ]
        }
      ],
      "source": [
        "# Example using the above functions\n",
        "mock_vocab = {\n",
        "    \"<pad>\": 0,\n",
        "    \"<sos>\": 1,\n",
        "    \"<eos>\": 2,\n",
        "    \"<unk>\": 3,\n",
        "    \"أحب\": 4,\n",
        "    \"تعلم\": 5,\n",
        "    \"البرمجة\": 6\n",
        "}\n",
        "text = \"أحب تعلم البرمجة\"\n",
        "numericalized_text = numericalize(text, tokenize_arabic, mock_vocab)\n",
        "print(numericalized_text)  # [token indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgU1zK7qp7Yn",
        "outputId": "bf8963eb-0983-40af-ed7b-b1390ac98a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "3\n",
            "3\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Create vocabularies\"\"\"\n",
        "# Prepare token iterators\n",
        "train_arabic_tokens = yield_tokens((example['arabic'] for example in train_data), tokenize_arabic)\n",
        "train_english_tokens = yield_tokens((example['english'] for example in train_data), tokenize_eng)\n",
        "\n",
        "# Build vocabularies\n",
        "# build_vocab_from_iterator takes in  iterator used to build vocab, min_freq needed to include token in the vocab, special symbols to add to vocab, and max tokens to create vocab from most frequent tokens\n",
        "arabic_vocab = build_vocab_from_iterator(train_arabic_tokens, min_freq=2, specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"], max_tokens=10000)\n",
        "english_vocab = build_vocab_from_iterator(train_english_tokens, min_freq=2, specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"], max_tokens=10000)\n",
        "\n",
        "# Set default index for padding token\n",
        "# will retrieve the index associated with the <unk> token\n",
        "# set_default_index is a torchtext method to define what index to use if a word is not in the vocab (aka an out-of-vocabulary token (OOV))\n",
        "# any unkonwn tokens will map to the \"<unk>\" index\n",
        "arabic_vocab.set_default_index(arabic_vocab[\"<unk>\"])\n",
        "english_vocab.set_default_index(english_vocab[\"<unk>\"])\n",
        "\n",
        "src_pad_idx = arabic_vocab[\"<pad>\"] # Get <pad> token indices from both vocabs\n",
        "trg_pad_idx = english_vocab[\"<pad>\"]\n",
        "print(src_pad_idx)\n",
        "print(trg_pad_idx)\n",
        "print(arabic_vocab[\"<unk>\"])\n",
        "print(english_vocab[\"<unk>\"])\n",
        "print(arabic_vocab[\"<sos>\"])\n",
        "print(english_vocab[\"<sos>\"])\n",
        "print(arabic_vocab[\"<eos>\"])\n",
        "print(english_vocab[\"<eos>\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Z4GCmdTtFeON"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# Custom class and functions used instead of outdated Field and BucketIterator\n",
        "# Class is used to fetch, batch, and iterate over the tokenized, numericalized data during training.\n",
        "# The DataLoader also provides convenient batching and shuffling, which are crucial for efficient model training.\n",
        "class TranslationDataset(Dataset):\n",
        "  def __init__(self, data, arabic_vocab, english_vocab, max_len=100):\n",
        "    self.data = data\n",
        "    self.arabic_vocab = arabic_vocab\n",
        "    self.english_vocab = english_vocab\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  # Method to fetch both the arabic and english sentence pairs at the idx-th index\n",
        "  def __getitem__(self, idx):\n",
        "    arabic_text = self.data[idx]['arabic']\n",
        "    english_text = self.data[idx]['english']\n",
        "\n",
        "    # Numericalize the text\n",
        "    arabic_numericalized = numericalize(arabic_text, tokenize_arabic, self.arabic_vocab)\n",
        "    english_numericalized = numericalize(english_text, tokenize_eng, self.english_vocab)\n",
        "\n",
        "    # Truncate if exceeding max_len\n",
        "    arabic_numericalized = arabic_numericalized[:self.max_len]\n",
        "    english_numericalized = english_numericalized[:self.max_len]\n",
        "    return torch.tensor(arabic_numericalized), torch.tensor(english_numericalized)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    # Pad sequences within the batch to the max length of the batch sequences\n",
        "    src_batch = pad_sequence(src_batch, padding_value=src_pad_idx, batch_first=False)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=trg_pad_idx, batch_first=False)\n",
        "    \"\"\"DEBUG\"\"\"\n",
        "    # print(f\"Padded src_batch: {src_batch}\")\n",
        "    # print(f\"Padded trg_batch: {trg_batch}\")\n",
        "\n",
        "    class Batch:\n",
        "      def __init__(self, src, trg):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "\n",
        "    batch = Batch(src_batch, trg_batch)\n",
        "    return batch\n",
        "\n",
        "train_dataset = TranslationDataset(train_data, arabic_vocab, english_vocab, max_len=100)\n",
        "valid_dataset = TranslationDataset(valid_data, arabic_vocab, english_vocab, max_len=100)\n",
        "test_dataset = TranslationDataset(test_data, arabic_vocab, english_vocab, max_len=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFwtOMzdOFw-"
      },
      "source": [
        "* MODEL ARCHITECTURE: Features separate embedding layers for both arabic and english inputs. Position embeddings are added to account for the order of tokens wihthin the sequence. The transformer comprises six encoder and six decoder layers, each utilizing eight attention heads for robust feature extraction. To combat overfitting, dropout is implemented throughout the model, and the output layyer is a linear layer that maps the transformer's outputs to the target vocabulary for english translation. This particular architecture is based on the paper Attention is All you Need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "330P2BRcFTnZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\"\"\"Defining Transformer Model\"\"\"\n",
        "class Transformer(nn.Module):\n",
        "  # Constructor for Transformer class\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads, # number of heads used for attention mechanism\n",
        "        num_encoder_layers, # number of encoder layers\n",
        "        num_decoder_layers, # number of decoder layers\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_len,\n",
        "        device\n",
        "    ):\n",
        "        # Defining neural network layers\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        \"\"\"Creates an embedding layer for the source(arabic) vocabulary\n",
        "           initializes a lookup table that maps each word index (from source vocab) to a dense vector representation of fixed size(aka embedding size)\n",
        "           src_vocab_size is specifying the totla number of unique words (tokens) in the source vocab\n",
        "           embedding_size specifies the size of the vector for each word; e.g. 256, then each word will be represented by a 256-dimensional vector\"\"\"\n",
        "\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size+1, embedding_size)\n",
        "\n",
        "        \"\"\" Createas embedding layer for positional information\n",
        "            max_len is the length of the sequences the model will handle\n",
        "            each position in a sequence (from 0 to max_len -1) gets a unique vector representation of size embedding_size\n",
        "            Transformers do not consider the iinput sequences (processes all tokens at once) positional embeddings are crucial for inejcting information\n",
        "            about the order of the tokens helping the model understand the sequence structure to learn relationships based on positions within a sentence\"\"\"\n",
        "\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size+1, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.device = device # set device\n",
        "        # Using PyTorch implementation of transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout,\n",
        "        )\n",
        "\n",
        "        # Fully connected (linear) layer\n",
        "        # Used to transform the output of the model (After processing through the network) into a format suitable for generating predictions for the target vocab\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout) # randomly zeros some of the elements of the input tensor; this is to prevent overfitting during training by randomly dropping (set to zero) a sepcified fraction of the neurons\n",
        "        self.src_pad_idx = src_pad_idx # just assigning the index for the padding token\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        \"\"\"Purpose: So the attention mechanism knows which tokens to ignore (i.e. padding and unkown tokens) when computing attention scores. Ensures only meanignful tokens contribute to the output\n",
        "          src shape: (src_len, N)\n",
        "          src_len is the length of the source sequences\n",
        "          N is the batch size (number of sequences processed in parallel)\n",
        "          transpose swaps the first dimension with the second dimension\n",
        "          creates a boolean mask: True if src is equal to the padding index \"\"\"\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        return src_mask # output for Pytorch (N, src_len)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "          # Ensure input is of the correct type\n",
        "        assert src.dtype == torch.long, \"Input src should be of type long.\"\n",
        "        assert trg.dtype == torch.long, \"Input trg should be of type long.\"\n",
        "\n",
        "        # Debug output\n",
        "        # print(\"Source Input Shape:\", src.shape)\n",
        "        # print(\"Target Input Shape:\", trg.shape)\n",
        "\n",
        "        # Validate indices\n",
        "        assert src.max() < self.src_word_embedding.num_embeddings, f\"Source indices out of range: {src.max().item()}\"\n",
        "        assert trg.max() < self.trg_word_embedding.num_embeddings, f\"Target indices out of range: {trg.max().item()}\"\n",
        "\n",
        "        # Get dimensions\n",
        "        src_seq_length, batch_size = src.shape\n",
        "        trg_seq_length, _ = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            # Creaates 1D tensor of integers ranging from 0 to src_seq_length -1.\n",
        "            torch.arange(0, src_seq_length) # i.e src_seq_length = 5 => [0, 1, 2, 3, 4, 5]\n",
        "            .unsqueeze(1) # transforms the tensor from 1D to 2D (adds dimension) [[0,1,2,3,4,5]]\n",
        "            .expand(src_seq_length, batch_size) # replicates the unsqueezed tensor to match the desired shape; each position value will be repeated \"trg_seq-length\" times across the new dimension and batch_size times across the other dimension\n",
        "            .to(self.device) # moves the tensor to the specified device (CPU or GPU)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(trg_seq_length, batch_size)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        # Compute embeddings\n",
        "        #\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=trg_mask\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "-IddSNAOJpJc"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, arabic_vocab, english_vocab, device, max_length=100):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Tokenize the input sentence\n",
        "    src_indices = numericalize(sentence, tokenize_arabic, arabic_vocab)\n",
        "    print(f\"Source: {src_indices}\")\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    trg_indices = [english_vocab[\"<sos>\"]]\n",
        "    trg_tensor = torch.tensor(trg_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    adjusted_max_length = min(max_length, len(trg_indices)*2)\n",
        "    for _ in range(adjusted_max_length):\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output = model(src_tensor, trg_tensor)\n",
        "      output_token = output.argmax(2)[-1, -1].item()\n",
        "      trg_indices.append(output_token)\n",
        "\n",
        "      if output_token  == english_vocab[\"<eos>\"]:\n",
        "        break\n",
        "    print(f\"Target: {trg_indices}\")\n",
        "    translated_tokens = [english_vocab.get_itos()[idx] for idx in trg_indices[1:]]\n",
        "\n",
        "    return ' '.join(translated_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WevvZennPjuU",
        "outputId": "7426b0b3-507e-4044-ae65-7b7214b33a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample numericalized source: [1, 3, 3, 3, 2]\n",
            "Sample numericalized target: [1, 103, 3, 2408, 2]\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample numericalized source:\", numericalize(\"some Arabic text\", tokenize_arabic, arabic_vocab))\n",
        "print(\"Sample numericalized target:\", numericalize(\"some English text\", tokenize_eng, english_vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T77YSA3omqq1"
      },
      "outputs": [],
      "source": [
        "# def bleu(model, test_data, arabic_vocab, english_vocab, device):\n",
        "#     total_score = 0\n",
        "#     for i in range(len(test_data)):\n",
        "#         # Debugging: Check that `test_data[i].trg` and `test_data[i].src` are accessible\n",
        "#         print(f\"Processing sentence {i+1}/{len(test_data)}\")\n",
        "\n",
        "#         # Get the target translation for the current sample\n",
        "#         try:\n",
        "#             reference_indices = numericalize(test_data[i].trg, tokenize_eng, english_vocab)\n",
        "#             reference = [english_vocab.get_itos()[idx] for idx in reference_indices]\n",
        "#         except KeyError as e:\n",
        "#             print(f\"KeyError for reference: {e}\")\n",
        "#             continue\n",
        "\n",
        "#         # Generate a translation using your model\n",
        "#         try:\n",
        "#             translation_indices = numericalize(translate_sentence(model, test_data[i].src, english_vocab, arabic_vocab, device), tokenize_eng, english_vocab)\n",
        "#             translation = [english_vocab.get_itos()[idx] for idx in translation_indices]\n",
        "#         except KeyError as e:\n",
        "#             print(f\"KeyError for translation: {e}\")\n",
        "#             continue\n",
        "\n",
        "#         # Calculate BLEU score for the current sample\n",
        "#         score = bleu_score([reference], translation)\n",
        "\n",
        "#         # Accumulate the scores\n",
        "#         total_score += score\n",
        "\n",
        "#     # Return the average score\n",
        "#     return total_score / len(test_data) if len(test_data) > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bleu(model, test_iterator, arabic_vocab, english_vocab, device):\n",
        "    total_score = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in test_iterator:\n",
        "        src_batch, trg_batch = batch.src, batch.trg\n",
        "        print(f\"Processing batch with source shape {src_batch.shape} and target shape {trg_batch.shape}\")\n",
        "\n",
        "        for i in range(trg_batch.size(0)):  # Iterate through each example in the batch\n",
        "            # Get the target translation for the current sample\n",
        "            reference_indices = numericalize(trg_batch[i].cpu().numpy(), tokenize_eng, english_vocab)  # Make sure to convert tensor to numpy array first\n",
        "            reference = [english_vocab.get_itos()[idx] for idx in reference_indices]\n",
        "\n",
        "            # Convert the source tensor back to a string for translation\n",
        "            source_sentence = ' '.join([arabic_vocab.get_itos()[idx.item()] for idx in src_batch[i] if idx != src_pad_idx])\n",
        "\n",
        "            # Generate a translation using the loaded model\n",
        "            translation_indices = numericalize(translate_sentence(model, source_sentence, arabic_vocab, english_vocab, device), tokenize_eng, english_vocab)\n",
        "            translation = [english_vocab.get_itos()[idx] for idx in translation_indices]\n",
        "\n",
        "            # Calculate BLEU score for the current sample\n",
        "            score = bleu_score([reference], translation)\n",
        "\n",
        "            # Accumulate the scores\n",
        "            total_score += score\n",
        "            total_samples += 1\n",
        "\n",
        "    # Return the average score\n",
        "    return total_score / total_samples if total_samples > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def bleu(model, test_data, arabic_vocab, english_vocab, device):\n",
        "#     total_score = 0\n",
        "#     tot\n",
        "#     for i in range(len(test_data)):\n",
        "#         # Debugging: Check that `test_data[i].trg` and `test_data[i].src` are accessible\n",
        "#         print(f\"\\nProcessing sentence {i+1}/{len(test_data)}\")\n",
        "\n",
        "#         # Get the target translation for the current sample\n",
        "#         print(f\"Target Translation:{test_data[i]['english']}\")\n",
        "#         reference_indices = numericalize(test_data[i][\"english\"], tokenize_eng, english_vocab)\n",
        "#         reference = [english_vocab.get_itos()[idx] for idx in reference_indices]\n",
        "\n",
        "#         # Generate a translation using the loaded model\n",
        "#         print(f\"Generated Translation:{test_data[i]['arabic']}\")\n",
        "#         translation_indices = numericalize(translate_sentence(model, test_data[i][\"arabic\"], english_vocab, arabic_vocab, device), tokenize_eng, english_vocab)\n",
        "#         translation = [english_vocab.get_itos()[idx] for idx in translation_indices]\n",
        "\n",
        "#         # Calculate BLEU score for the current sample\n",
        "#         score = bleu_score([reference], translation)\n",
        "\n",
        "#         # Accumulate the scores\n",
        "#         total_score += score\n",
        "\n",
        "#     # Return the average score\n",
        "#     return total_score / len(test_data) if len(test_data) > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example for loading a checkpoint\n",
        "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\"):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    print(f\"Checkpoint loaded: Epoch {epoch}\")\n",
        "    return epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG6DIxk1duqN"
      },
      "source": [
        "## Setting up Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2aCFZjAbFvQj",
        "outputId": "fd2554a1-4dd0-476d-b998-73a92246febe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint loaded: Epoch 32\n",
            "[Epoch 0 / 100]\n",
            "Loss at iteration 0: 2.739516496658325\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_model = True\n",
        "save_model = True\n",
        "\n",
        "# Training Hyperparameters\n",
        "num_epochs = 100\n",
        "learning_rate = 3e-4\n",
        "batch_size = 256\n",
        "\n",
        "# Model Hyperparameters\n",
        "src_vocab_size = len(arabic_vocab)\n",
        "trg_vocab_size = len(english_vocab)\n",
        "embedding_size = 256\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3 #decrease layers to reduce model complexity and increase speed \n",
        "num_decoder_layers = 3\n",
        "dropout = 0.1\n",
        "max_len = 100 # sentence length; have to either delete sentences longer than 100 or increase max length\n",
        "forward_expansion = 2 # determines the size of the hidden layers for the forward pass/increases or decreases model's hidden layer size\n",
        "src_pad_idx = arabic_vocab[\"<pad>\"] # Get <pad> token indices from both vocabs\n",
        "trg_pad_idx = english_vocab[\"<pad>\"]\n",
        "\n",
        "writer = SummaryWriter(\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "#     (train_data, valid_data, test_data),\n",
        "#     batch_size=batch_size,\n",
        "#     sort_within_batch=True,\n",
        "#     sort_key=lambda x: len(x.src),\n",
        "#     device=device,\n",
        "# )\n",
        "# Loading large data in manageable batch sizes\n",
        "train_iterator = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn = collate_fn) # Shuffle is on so at the start of each epoch the data is shuffled for learning\n",
        "valid_iterator = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn = collate_fn)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn = collate_fn)\n",
        "\n",
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_len,\n",
        "    device,\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "\n",
        "if load_model:\n",
        "     epoch = load_checkpoint(model, optimizer, path=\"my_checkpoint.pth\")\n",
        "     \n",
        "sentence = \"وَهُنَّ يَشْفَعْنَ إِلَيْهِ\" #\"And they will intercede for him.\"\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    if save_model:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch\n",
        "        }\n",
        "        torch.save(checkpoint, \"my_checkpoint.pth\")\n",
        "\n",
        "    # # model.eval()\n",
        "    # translated_sentence = translate_sentence(\n",
        "    #     model, sentence, arabic_vocab, english_vocab, device, max_length=100\n",
        "    # )\n",
        "\n",
        "    # print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device) # get source batch from train iterator\n",
        "        target = batch.trg.to(device) # get target batch from train iterator\n",
        "\n",
        "        # Replace out-of-range indices with UNK_INDEX\n",
        "        unk_index = arabic_vocab[\"<unk>\"]\n",
        "        unk_index_eng = english_vocab[\"<unk>\"]\n",
        "        # Get the maximum index in inp_data\n",
        "        max_src_index = inp_data.max().item()\n",
        "        max_trg_index = target[:-1].max().item()\n",
        "        # Check if the maximum index is out of range\n",
        "        if max_src_index >= src_vocab_size:\n",
        "            print(f\"Out-of-range indices detected. Max index: {max_src_index}, Replacing with <unk> index.\")\n",
        "            # Replace out-of-range indices with the UNK index\n",
        "            inp_data = torch.where(inp_data >= src_vocab_size, torch.tensor(unk_index, device=device), inp_data)\n",
        "        # Check if the maximum index is out of range\n",
        "        if max_trg_index >= trg_vocab_size:\n",
        "            print(f\"Out-of-range indices detected. Max index: {max_trg_index}, Replacing with <unk> index.\")\n",
        "            # Replace out-of-range indices with the UNK index\n",
        "            target = torch.where(target >= trg_vocab_size, torch.tensor(unk_index_eng, device=device), target)\n",
        "\n",
        "        if inp_data.max() >= src_vocab_size:\n",
        "            print(f\"Input data index out of range: {inp_data.max()}\")\n",
        "        if target.max() >= trg_vocab_size:\n",
        "            print(f\"Target data index out of range: {target.max()}\")\n",
        "        else:\n",
        "          # forward propagation\n",
        "          output = model(inp_data, target[:-1]) # pass in the input data(arabic) and target data(english) through the model and remove the last token in english\n",
        "        #   if i % 10 == 0:\n",
        "        #         # Get the actual output of the model\n",
        "        #     test_output = output.argmax(2)\n",
        "\n",
        "        #     # Use teacher forcing to generate the source and target sentences\n",
        "        #     src_sentence = \" \".join([arabic_vocab.get_itos()[idx] for idx in inp_data.cpu().numpy()[0]])\n",
        "        #     trg_sentence = \" \".join([english_vocab.get_itos()[idx] for idx in target.cpu().numpy()[0]])\n",
        "\n",
        "        #     # Use the actual output of the model to generate the translated sentence\n",
        "        #     translated_sentence = \" \".join([english_vocab.get_itos()[idx] for idx in test_output.cpu().numpy()[0]])\n",
        "\n",
        "        #     print(f\"Source sentence: {src_sentence}\")\n",
        "        #     print(f\"Target sentence: {trg_sentence}\")\n",
        "        #     print(f\"Translated sentence: {translated_sentence}\")\n",
        "          output = output.reshape(-1, output.shape[2])\n",
        "          target = target[1:].reshape(-1)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = loss_fn(output, target)\n",
        "          loss.backward()\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "          optimizer.step()\n",
        "          # Print loss every 10 training examples\n",
        "          if i % 100 == 0:\n",
        "            print(f\"Loss at iteration {i}: {loss.item()}\")\n",
        "          writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "          step += 1\n",
        "#           score = bleu(model, test_iterator, arabic_vocab, english_vocab, device)\n",
        "#           print(f\"Bleu score: {score}\")\n",
        "# print(f\"Bleu score {score*100:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLyc0RKPOJNg"
      },
      "source": [
        "* TRAINING PROCESS: Involves defining several key hyperparameters, including a learning rate of 3e-4, a batch size of 32 and a total of five epochs. The loss funciton used is Cross entropy loss whihc ignores padding tokens to ensure accurate evaluation. During each iteration of the training loop, the model processes batches of data, performs forward passes to generate predictions, calculates the lossses and updataes the model weights through backpropagation. This is the typical training process used by various examples and tutorials.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9RTIgOSOL2R"
      },
      "source": [
        "* EVALUATION: Use metrics like BLEU score to measure the quality of the translations produced by the model. Depending on the results, potential improvements invovle fine tuning the hyperparameters or implementing data augmentation techniques to enrich the training data.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mytestenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
